当前的物体检测算法为了保证召回率，对于同一个真实物体往往会有多于1个的候选框输出。由于多余的候选框会影响检测精度，因此需要利用NMS过滤掉重叠的
候选框，得到最佳的预测输出。

基本的NMS方法，利用得分高的边框抑制得分低且重叠程度高的边框。然而基本的NMS存在一些缺陷，简单地过滤掉得分低且重叠度高的边框可能会导致漏检等问题。
针对此问题陆续产生了一系列改进的方法，如Soft NMS、Softer NMS及IoU-Net等。

目标检测主要的任务：1）对象是什么？2）对象在哪里？其中，对象是什么主要分清楚对象的类别。而对象在哪里，需要寻找这个对象在图像中的位置。回归损失
问题就是探讨如何更好地学习对象在哪里。当然最近anchor free的方法有很多，但是主流应用上目前还是基于anchor的方式。

对于有先验框的目标检测，位置是通过学习给定的先验框和真实目标框的距离来进行预测。而这个距离的刻画主要通过距离公式来度量，比如曼哈顿距离L1和
欧式距离L2。

那么利用欧式距离来计算存在什么问题呢？继续往下看，下面将详细介绍。一起来看攻克目标检测难点秘籍二：非极大值抑制和回归损失优化之路。


1 NMS：非极大值抑制优化

为了保证物体检测的召回率，在Faster RCNN或者SSD网络的计算输出中，通常都会有不止一个候选框对应同一个真实物体。如下图左图是人脸检测的候选框结果，
每个边界框有一个置信度得分(confidence score)，如果不使用非极大值抑制，就会有多个候选框出现。右图是使用非极大值抑制之后的结果，符合我们人脸检测
的预期结果。

非极大值抑制，顾名思义就是抑制不是极大值的边框，这里的抑制通常是直接去掉冗余的边框。这个过程涉及以下两个量化指标。
1）预测得分：NMS假设一个边框的预测得分越高，这个框就要被优先考虑，其他与其重叠超过一定程度的边框要被舍弃，非极大值即是指得分的非极大值。
2）IoU：在评价两个边框的重合程度时，NMS使用了IoU这个指标。如果两个边框的IoU超过一定阈值时，得分低的边框会被舍弃。阈值通常会取0.5或者0.7。

NMS算法输入包含了所有预测框的得分、左上点坐标、右下点坐标一共5个预测量，以及一个设定的IoU阈值。算法具体流程如下：
（1）按照得分，对所有边框进行降序排列，记录下排列的索引order，并新建一个列表keep，作为最终筛选后的边框索引结果。
（2）将排序后的第一个边框置为当前边框，并将其保留到keep中，再求当前边框与剩余所有框的IoU。
（3）在order中，仅仅保留IoU小于设定阈值的索引，重复第（2）步，直到order中仅仅剩余一个边框，则将其保留到keep中，退出循环，NMS结束。

利用PyTorch，可以很方便地实现NMS模块。

def nms(self, bboxes, scores, thresh=0.5):

    x1 = bboxes[:,0]
    y1 = bboxes[:,1]
    x2 = bboxes[:,2]
    y2 = bboxes[:,3]
    areas = (x2-x1+1)*(y2-y1+1) 
    _, order = scores.sort(0, descending=True)
    keep = []

    while order.numel() > 0:
        if order.numel() == 1:
            i = order.item()
            keep.append(i)
            break
        else:
            i = order[0].item()
            keep.append(i)
        xx1 = x1[order[1:]].clamp(min=x1[i])
        yy1 = y1[order[1:]].clamp(min=y1[i])
        xx2 = x2[order[1:]].clamp(max=x2[i])
        yy2 = y2[order[1:]].clamp(max=y2[i])
        inter = (xx2-xx1).clamp(min=0) * (yy2-yy1).clamp(min=0)
        iou = inter / (areas[i]+areas[order[1:]]-inter)
        idx = (iou <= threshold).nonzero().squeeze()
        if idx.numel() == 0:
            break
        order = order[idx+1]

    return torch.LongTensor(keep)

NMS方法虽然简单有效，但在更高的目标检测需求下，也存在如下缺点：
1.将得分较低的边框强制性地去掉，如果物体出现较为密集时，本身属于两个物体的边框，其中得分较低的也有可能被抑制掉，降低了模型的召回率。
2.速度：NMS的实现存在较多的循环步骤，GPU的并行化实现不是特别容易，尤其是预测框较多时，耗时较多。   
3.将得分作为衡量指标。NMS简单地将得分作为一个边框的置信度，但在一些情况下，得分高的边框不一定位置更准。
4.阈值难以确定。过高的阈值容易出现大量误检，而过低的阈值则容易降低模型的召回率，超参很难确定。

1.1 Soft NMS：抑制得分

NMS方法虽有效过滤了重复框，但也容易将本属于两个物体框中得分低的框抑制掉，从而降低了召回率。造成这种现象的原因在于NMS的计算公式。

image.png
